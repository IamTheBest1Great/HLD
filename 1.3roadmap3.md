# The Complete Guide to Mastering High-Level Design Interviews

## Your Pattern-Based Roadmap to FAANG-Level System Design

Since you're already strong in **DSA + DP patterns**, you should approach **HLD (High-Level Design)** the same way: **pattern-based mastery + structured tracking + iterative refinement**.

This comprehensive guide covers everything from foundational concepts to advanced Staff-level topics, with categorized practice questions organized for systematic learning.

---

# üéØ PHASE 0 ‚Äî Understanding What HLD Interviews Actually Test

Before diving into preparation, understand what top companies (Google, Amazon, Meta, Netflix, Microsoft) are actually evaluating.

## What Interviewers Look For

| Evaluation Criteria | What They're Testing |
|---------------------|----------------------|
| **Requirement Clarification** | Can you handle ambiguity and scope the problem correctly? |
| **System Decomposition** | Can you break complex systems into manageable components? |
| **Scalability Reasoning** | Do you understand how systems behave under load? |
| **Trade-off Analysis** | Can you justify why you chose one solution over another? |
| **Data Modeling** | Do you understand database design at scale? |
| **Bottleneck Identification** | Can you find and fix weak points in your architecture? |
| **Communication Clarity** | Can you explain complex ideas clearly and collaboratively? |

## What They're NOT Testing
- ‚ùå Perfect architecture with every detail
- ‚ùå Memorized diagrams from blog posts
- ‚ùå Framework or tool-specific knowledge
- ‚ùå Getting the "right answer" (there is none)

## What They ARE Testing
- ‚úÖ **Thinking under ambiguity** ‚Äî Your ability to make reasonable assumptions and defend them
- ‚úÖ **Structured problem-solving** ‚Äî Following a logical framework
- ‚úÖ **Depth vs. breadth balance** ‚Äî Knowing when to dive deep vs. stay high-level

---

# üß† PHASE 1 ‚Äî Build Strong Foundational Knowledge (Weeks 1-3)

Before designing complex systems, master these atomic building blocks. You cannot design a skyscraper if you don't understand bricks.

## Core Concepts You Must Know Cold

### 1Ô∏è‚É£ Networking Fundamentals

| Concept | What to Understand |
|---------|---------------------|
| **HTTP/HTTPS** | Request-response model, methods (GET, POST, PUT, DELETE), status codes, headers |
| **TCP vs UDP** | Connection-oriented vs connectionless, reliability vs speed trade-offs |
| **DNS** | Domain resolution process, caching, latency implications |
| **Load Balancers** | Layer 4 (transport) vs Layer 7 (application), algorithms (Round Robin, Least Connections, IP Hash) |
| **CDN** | Edge caching, geographic distribution, pull vs push strategies |
| **WebSockets** | Full-duplex communication, use cases (chat, live updates) |
| **gRPC** | High-performance RPC, protocol buffers, streaming |
| **REST** | Resource-based APIs, statelessness, HTTP methods |

### 2Ô∏è‚É£ Scaling Fundamentals

| Concept | Explanation |
|---------|-------------|
| **Vertical Scaling** | Adding more power (CPU, RAM) to existing servers ‚Äî simpler but has limits |
| **Horizontal Scaling** | Adding more servers ‚Äî complex but theoretically infinite |
| **Stateless Services** | No session data stored locally ‚Äî easily scalable |
| **Stateful Services** | Maintain session state ‚Äî harder to scale, needs sticky sessions or external storage |
| **Reverse Proxy** | Sits between clients and servers ‚Äî provides caching, load balancing, security |

### 3Ô∏è‚É£ Database Deep Dive

#### SQL vs NoSQL Decision Matrix

| Factor | Choose SQL | Choose NoSQL |
|--------|------------|--------------|
| **Data Structure** | Structured, fixed schema | Unstructured, semi-structured, flexible schema |
| **Relationships** | Complex relationships, joins | Few relationships, denormalized data |
| **Consistency Needs** | Strong consistency (ACID) | Eventual consistency acceptable (BASE) |
| **Scale** | Vertical scaling (with some horizontal) | Horizontal scaling by design |
| **Query Complexity** | Complex queries, aggregations | Simple key-based lookups |
| **Use Cases** | Financial systems, inventory | Social feeds, real-time analytics |

#### NoSQL Types

| Type | Examples | Use Case |
|------|----------|----------|
| **Key-Value** | Redis, Memcached, DynamoDB | Caching, session storage |
| **Document** | MongoDB, CouchDB | Content management, catalogs |
| **Wide-Column** | Cassandra, HBase | Time-series data, analytics |
| **Graph** | Neo4j, Amazon Neptune | Social networks, recommendations |

#### Critical Database Concepts

| Concept | What to Understand |
|---------|---------------------|
| **Indexing** | B-Tree vs Hash indexes, composite indexes, trade-offs (read speed vs write speed) |
| **Replication** | Master-Slave (read scaling), Master-Master (write scaling), synchronous vs asynchronous |
| **Sharding/Partitioning** | Horizontal partitioning, shard keys, rebalancing challenges |
| **Consistency Models** | Strong, Eventual, Causal, Read-your-writes |
| **ACID vs BASE** | Atomicity, Consistency, Isolation, Durability vs Basically Available, Soft state, Eventual consistency |

### 4Ô∏è‚É£ Caching Strategies

| Strategy | How It Works | Pros | Cons |
|----------|--------------|------|------|
| **Cache-Aside** | Application checks cache first, loads from DB on miss | Simple, cache only what's needed | Cache miss penalty |
| **Read-Through** | Cache sits between app and DB, loads on miss | App code simpler | Cache must understand DB |
| **Write-Through** | Data written to cache and DB simultaneously | Consistency, no stale data | Higher write latency |
| **Write-Back** | Write to cache only, async write to DB | Fast writes | Risk of data loss if cache fails |
| **Write-Around** | Write directly to DB, cache on read | Prevents cache pollution | Cache miss on recent writes |

#### Eviction Policies

| Policy | Description |
|--------|-------------|
| **LRU (Least Recently Used)** | Evict items not accessed for longest time |
| **LFU (Least Frequently Used)** | Evict items accessed least often |
| **FIFO (First In, First Out)** | Evict oldest items regardless of usage |
| **TTL (Time-To-Live)** | Evict after fixed time |

### 5Ô∏è‚É£ Messaging and Queues

| Concept | Explanation |
|---------|-------------|
| **Message Queue** | Point-to-point communication (RabbitMQ, SQS) |
| **Pub/Sub** | One-to-many broadcast (Kafka, SNS) |
| **Queue vs Stream** | Queue: message removed after consumption; Stream: messages persist |
| **At-least-once delivery** | Messages may be duplicated but never lost |
| **Exactly-once delivery** | Hard to achieve, requires idempotent consumers |
| **Message ordering** | FIFO queues vs standard queues |

### 6Ô∏è‚É£ Storage Systems

| Storage Type | Examples | Use Case |
|--------------|----------|----------|
| **Object Storage** | AWS S3, Google Cloud Storage | Files, images, videos ‚Äî unlimited scale |
| **Block Storage** | AWS EBS, SSD | Databases, high-performance needs |
| **File Storage** | AWS EFS, NFS | Shared file systems |
| **Blob Storage** | Azure Blob | Large unstructured data |

### 7Ô∏è‚É£ Reliability Patterns

| Pattern | Purpose | How It Works |
|---------|---------|--------------|
| **Rate Limiting** | Prevent abuse | Token bucket, Leaky bucket, Sliding window |
| **Circuit Breaker** | Prevent cascading failures | Open (failing fast), Closed (normal), Half-Open (testing) |
| **Retry with Backoff** | Handle transient failures | Exponential backoff, jitter |
| **Idempotency** | Safe retries | Idempotency keys, same request ‚Üí same response |
| **Bulkhead** | Isolate failures | Partition resources by service/tenant |
| **Health Checks** | Detect failures | Heartbeat endpoints, passive vs active |

### 8Ô∏è‚É£ The CAP Theorem

You cannot have all three. You must choose two.

```
        Consistency (C)
              /\
             /  \
            /    \
           /      \
          /        \
         /          \
        /            \
       /              \
      /                \
Partition Tolerance (P) ‚Äî‚Äî‚Äî Availability (A)
```

| Choice | Description | Examples |
|--------|-------------|----------|
| **CP (Consistency + Partition Tolerance)** | Sacrifice availability during partitions | Traditional RDBMS, HBase |
| **AP (Availability + Partition Tolerance)** | Sacrifice consistency during partitions | Cassandra, DynamoDB |
| **CA (Consistency + Availability)** | Not possible in distributed systems | (Single-node systems only) |

#### Modern Extension: PACELC
- **During Partition (P)** ‚Äî trade-off between Availability and Consistency
- **Else (E)** ‚Äî trade-off between Latency and Consistency

---

## Recommended Resources for Phase 1

| Resource | Why |
|----------|-----|
| **Designing Data-Intensive Applications** | The bible ‚Äî read replication, partitioning, consistency chapters |
| **System Design Interview ‚Äî An Insider's Guide (Vol 1 & 2)** | Best practical introduction |
| **Grokking the System Design Interview** | Structured course format |
| **High Scalability Blog** | Real-world architecture case studies |

---

# üèó PHASE 2 ‚Äî Master the Interview Framework (Your 45-Minute Template)

A structured framework keeps you from rambling or designing the wrong system. Practice applying this exact 6-step framework to every problem.

## The Complete Interview Framework

### Step 1 ‚Äî Clarify Requirements (5-10 minutes)

Never jump straight into designing. Ask questions to define scope.

| Type | Questions to Ask |
|------|------------------|
| **Functional Requirements** | What should the system do? What are the core features? What's in scope vs out of scope? |
| **Non-Functional Requirements** | How many users? Read/write ratio? Expected latency? Consistency needs? Availability SLA? |
| **Scale** | Daily Active Users (DAU)? Total users? Data retention period? Geographic distribution? |

**Example for Twitter:**
- *Functional*: Post tweet, follow users, view timeline, search tweets
- *Non-functional*: Timeline load <500ms, 99.9% availability, 200M DAU, 60:40 read:write ratio
- *Out of scope*: Ads, analytics, recommendations

### Step 2 ‚Äî Back-of-the-Envelope Estimation (5 minutes)

Calculate scale to inform design choices.

| What to Estimate | How | Example (Twitter-scale) |
|------------------|-----|------------------------|
| **Traffic (QPS)** | DAU √ó actions per user / seconds in day | 200M DAU √ó 2 tweets/day / 86400 ‚âà 4,600 writes/sec |
| **Peak QPS** | Average √ó 2-3x | 4,600 √ó 3 ‚âà 13,800 writes/sec peak |
| **Storage** | Daily √ó retention √ó size per item | 500M tweets/day √ó 5 years √ó 1KB ‚âà 912 TB |
| **Bandwidth** | Data transferred per second | 4,600 writes/sec √ó 1KB = 4.6 MB/s upload |
| **Cache Size** | 20% of daily active data | 20% √ó 500M tweets = 100M tweets in cache |

### Step 3 ‚Äî API Design (5 minutes)

Define the contract between clients and servers.

| API Element | What to Include | Example |
|-------------|-----------------|---------|
| **Endpoints** | RESTful resource paths | `POST /tweet`, `GET /timeline/{user_id}` |
| **Request/Response** | JSON structure | `{ "text": "Hello", "media_ids": [] }` |
| **Authentication** | How users are identified | JWT in Authorization header |
| **Pagination** | For list endpoints | `limit`, `offset` or `cursor` params |

### Step 4 ‚Äî High-Level Architecture (10 minutes)

Draw the "happy path" from client to database.

```
Client (Mobile/Web)
    ‚Üì
DNS (Route53/Cloudflare)
    ‚Üì
CDN (CloudFront) ‚Üê‚îÄ‚îÄ‚îÄ‚îê (Static assets)
    ‚Üì                ‚îÇ
Load Balancer (ELB/NGINX)
    ‚Üì
Application Servers (Stateless, auto-scaling)
    ‚Üì                              ‚Üì
Cache (Redis/Memcached)    Message Queue (Kafka)
    ‚Üì                              ‚Üì
Database (Primary)          Worker Servers
    ‚Üì                              ‚Üì
Database (Replicas)         Database (Analytics)
```

### Step 5 ‚Äî Deep Dive on Key Components (15 minutes)

The interviewer will ask you to zoom in on 1-2 critical areas. This is where you demonstrate depth.

#### Common Deep Dive Areas

| Area | Questions to Address |
|------|---------------------|
| **Database** | Schema design? Sharding key? Indexing strategy? Read replicas? |
| **Caching** | What to cache? Cache strategy? Eviction policy? Cache invalidation? |
| **Data Flow** | End-to-end flow for a key feature? Where are bottlenecks? |
| **Scaling** | How does this component scale horizontally? Single points of failure? |
| **Consistency** | Strong or eventual? How to handle conflicts? |
| **Real-time** | WebSockets? Polling? Server-Sent Events? |

### Step 6 ‚Äî Identify Bottlenecks and Trade-offs (5 minutes)

Critically evaluate your own design. Show you can find flaws.

| Question to Ask Yourself | Why It Matters |
|--------------------------|----------------|
| Where is the single point of failure? | Shows fault tolerance awareness |
| What happens if traffic spikes 10x? | Shows scalability thinking |
| How do we handle network partitions? | Shows distributed systems knowledge |
| What's the biggest bottleneck? | Shows prioritization |
| Why did you choose X over Y? | Shows trade-off analysis |

---

# üìö PHASE 3 ‚Äî Pattern-Based Practice (The Core of Your Preparation)

Like Dynamic Programming, HLD has recurring patterns. Master these patterns, and you can solve any variant.

## Pattern Classification Framework

Each question is categorized by:
- **Pattern** ‚Äî The fundamental architectural pattern
- **Sub-pattern** ‚Äî Specific variant
- **Primary Challenge** ‚Äî What makes this question hard
- **Read/Write Ratio** ‚Äî Helps choose database and caching strategy
- **Consistency Needs** ‚Äî Strong vs Eventual
- **Real-time Requirements** ‚Äî Yes/No
- **Key Technologies** ‚Äî What components you'll likely use

---

# üü¢ PATTERN 1 ‚Äî CRUD / Storage-Based Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Data persistence at scale, efficient retrieval |
| **Typical Ratio** | Variable (read-heavy or write-heavy) |
| **Consistency** | Often strong for critical data |
| **Key Components** | Database, Cache, Object Storage |
| **Sub-patterns** | Basic CRUD, Scalable CRUD, Multi-tenant CRUD |

---

### Question 1.1: Design URL Shortener (TinyURL/Bitly)

| Category | Details |
|----------|---------|
| **Pattern** | CRUD / Storage-Based |
| **Sub-pattern** | Key-Value Store |
| **Primary Challenge** | Generating unique short keys, handling collisions, redirect speed |
| **Read/Write Ratio** | Heavy Read (100:1 reads to writes) |
| **Consistency Needs** | Strong (redirects must go to correct URL) |
| **Real-time** | No |
| **Key Technologies** | Base62 encoding, Hash functions, Redis cache, Relational DB |

**Key Decisions:**
- **Key generation**: Base62 encoding of unique ID, or hash of URL + collision handling
- **Database**: Relational for mapping, NoSQL for analytics
- **Caching**: Cache popular URLs in Redis (LRU eviction)
- **Scaling**: Consistent hashing for cache nodes, read replicas for DB

---

### Question 1.2: Design Pastebin

| Category | Details |
|----------|---------|
| **Pattern** | CRUD / Storage-Based |
| **Sub-pattern** | Key-Value + Expiry |
| **Primary Challenge** | Handling large text content, expiration, syntax highlighting |
| **Read/Write Ratio** | Read-heavy (80:20) |
| **Consistency Needs** | Strong (users expect their paste to be there) |
| **Real-time** | No |
| **Key Technologies** | Object storage for content, Metadata DB, CDN for syntax JS |

**Key Decisions:**
- **Storage**: Store content in S3/Blob storage, metadata in SQL DB
- **Expiry**: Background workers delete expired pastes
- **Caching**: Cache recent pastes in Redis
- **Unique ID**: Similar to URL shortener

---

### Question 1.3: Design Online Bookstore

| Category | Details |
|----------|---------|
| **Pattern** | CRUD / Storage-Based |
| **Sub-pattern** | E-commerce Catalog |
| **Primary Challenge** | Product search, inventory tracking, recommendations |
| **Read/Write Ratio** | Read-heavy (90:10) |
| **Consistency Needs** | Strong for inventory, eventual for catalog |
| **Real-time** | No |
| **Key Technologies** | Search index (Elasticsearch), SQL for inventory, Redis for sessions |

**Key Decisions:**
- **Catalog**: Denormalized document store (MongoDB) for product details
- **Search**: Elasticsearch for full-text search
- **Inventory**: SQL with ACID for accurate counts
- **Caching**: Product pages in Redis

---

### Question 1.4: Design Ticket Booking System (BookMyShow/Ticketmaster)

| Category | Details |
|----------|---------|
| **Pattern** | CRUD / Storage-Based |
| **Sub-pattern** | High-Concurrency Booking |
| **Primary Challenge** | Preventing double-booking, handling seat locks, high traffic for popular events |
| **Read/Write Ratio** | Mixed (read to browse, write to book) |
| **Consistency Needs** | Strong (must prevent overselling) |
| **Real-time** | Yes (seat availability updates) |
| **Key Technologies** | SQL with row-level locking, Redis for temporary holds, Message queues |

**Key Decisions:**
- **Concurrency control**: Optimistic locking with version numbers, or pessimistic row locks
- **Seat holds**: Temporary locks in Redis with TTL
- **Database**: SQL for ACID transactions
- **Scaling**: Shard by event ID, read replicas for browsing

---

### Question 1.5: Design Dropbox / Google Drive

| Category | Details |
|----------|---------|
| **Pattern** | CRUD / Storage-Based |
| **Sub-pattern** | File Storage & Sync |
| **Primary Challenge** | Large file uploads/downloads, sync across devices, conflict resolution |
| **Read/Write Ratio** | Mixed (depends on user behavior) |
| **Consistency Needs** | Strong (files should be consistent across devices) |
| **Real-time** | Yes (sync notifications) |
| **Key Technologies** | Object storage (S3), Metadata DB, Delta sync, WebSockets |

**Key Decisions:**
- **File storage**: Chunk files into blocks, store in S3
- **Metadata**: SQL DB for file hierarchy, permissions
- **Deduplication**: Hash-based block deduplication
- **Delta sync**: Only sync changed blocks
- **Conflict resolution**: Last-write-wins or create copies

---

# üü¢ PATTERN 2 ‚Äî Feed / Timeline Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Generating personalized timelines efficiently |
| **Typical Ratio** | Read-heavy (timeline views) with write spikes |
| **Consistency** | Eventual (slight delays acceptable) |
| **Key Components** | Message queues, Fan-out services, Cache |
| **Sub-patterns** | Push model, Pull model, Hybrid |

**The Core Concept: Fan-out**

| Model | How It Works | Pros | Cons |
|-------|--------------|------|------|
| **Push (Fan-out on Write)** | When user posts, push to all followers' timelines | Fast reads (pre-computed) | Writes expensive for celebrities |
| **Pull (Fan-out on Read)** | When user views timeline, fetch from followed users | Efficient writes | Slow reads (need to merge) |
| **Hybrid** | Push for regular users, pull for celebrities | Best of both | Complex logic |

---

### Question 2.1: Design Twitter / X

| Category | Details |
|----------|---------|
| **Pattern** | Feed / Timeline |
| **Sub-pattern** | Social Media Feed |
| **Primary Challenge** | Handling celebrity accounts with millions of followers, real-time updates |
| **Read/Write Ratio** | Read-heavy (timeline views) |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes (new tweets appear immediately) |
| **Key Technologies** | Hybrid fan-out, Redis cache, Cassandra for tweets, Kafka |

**Key Decisions:**
- **Fan-out strategy**: Hybrid ‚Äî push for regular users (<5k followers), pull for celebrities
- **Timeline generation**: Pre-compute timelines in Redis lists
- **Tweet storage**: Cassandra for write scalability
- **Social graph**: Graph DB or MySQL for follows

**Data Models:**
```
User: user_id, name, follower_count
Tweet: tweet_id, user_id, content, timestamp, media_ids
Timeline: user_id, list of (tweet_id, timestamp) (pre-computed in Redis)
Follow: user_id, follower_id
```

---

### Question 2.2: Design Instagram Feed

| Category | Details |
|----------|---------|
| **Pattern** | Feed / Timeline |
| **Sub-pattern** | Media-Heavy Feed |
| **Primary Challenge** | Handling images/videos, ranking feed by relevance |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes |
| **Key Technologies** | CDN for media, Push fan-out, Ranking service |

**Key Decisions:**
- **Media storage**: Images in CDN, metadata in DB
- **Feed generation**: Push-based with ranking algorithm
- **Ranking factors**: Recency, engagement, relationships
- **Caching**: Pre-computed ranked feeds in Redis

---

### Question 2.3: Design Facebook News Feed

| Category | Details |
|----------|---------|
| **Pattern** | Feed / Timeline |
| **Sub-pattern** | Algorithmic Feed |
| **Primary Challenge** | Complex ranking algorithm, diverse content types (posts, photos, links, videos) |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes |
| **Key Technologies** | Ranking service, ML models, Graph DB, Cassandra |

**Key Decisions:**
- **Content types**: Unified feed with mixed content
- **Ranking**: ML model with features (affinity, weight, time decay)
- **Storage**: Graph for social connections, Cassandra for feed items
- **Fan-out**: Push to active users, pull for inactive

---

### Question 2.4: Design LinkedIn Feed

| Category | Details |
|----------|---------|
| **Pattern** | Feed / Timeline |
| **Sub-pattern** | Professional Network Feed |
| **Primary Challenge** | Feed from 2nd/3rd degree connections, company pages, sponsored content |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | No (less real-time pressure) |
| **Key Technologies** | Pull-based feed, Graph DB, Relevance scoring |

**Key Decisions:**
- **Fan-out**: Pull-based due to large number of potential connections
- **Relevance**: Score based on connection strength, content type
- **Storage**: Graph DB for network, document store for posts

---

# üü¢ PATTERN 3 ‚Äî Real-Time Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Low-latency bidirectional communication, connection management |
| **Typical Ratio** | Variable (many small messages) |
| **Consistency** | Often eventual (message ordering matters) |
| **Key Components** | WebSockets, Pub/Sub, Presence servers |
| **Sub-patterns** | 1:1 Chat, Group Chat, Live Updates |

---

### Question 3.1: Design Chat System / WhatsApp

| Category | Details |
|----------|---------|
| **Pattern** | Real-Time |
| **Sub-pattern** | 1:1 and Group Messaging |
| **Primary Challenge** | Message delivery, ordering, last seen, read receipts, multi-device sync |
| **Read/Write Ratio** | Write-heavy (many messages) |
| **Consistency Needs** | Strong ordering per conversation |
| **Real-time** | Yes |
| **Key Technologies** | WebSockets, Message queues, Cassandra, Redis for presence |

**Key Decisions:**
- **Connection management**: WebSocket servers with sticky sessions
- **Message storage**: Cassandra for write scalability
- **Message ordering**: Client timestamps + server sequence IDs
- **Read receipts**: Pub/sub to notify senders
- **Last seen**: Redis with TTL
- **Multi-device**: Sync via server-stored message history

**Architecture:**
```
Client ‚Üí WebSocket Gateway ‚Üí Message Service ‚Üí Kafka ‚Üí Persistence
                                       ‚Üì
                                Notification Service
                                       ‚Üì
                                 Push Notifications
```

---

### Question 3.2: Design Discord / Slack

| Category | Details |
|----------|---------|
| **Pattern** | Real-Time |
| **Sub-pattern** | Group Chat with Channels |
| **Primary Challenge** | Large groups, voice channels, permissions, thread support |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Eventual for messages, strong for permissions |
| **Real-time** | Yes (voice requires UDP) |
| **Key Technologies** | WebSockets, WebRTC for voice, Cassandra, Redis |

**Key Decisions:**
- **Channels**: Separate message streams per channel
- **Voice**: WebRTC with SFU (Selective Forwarding Unit) architecture
- **Permissions**: Role-based access control in SQL
- **Message history**: Time-series DB for channel messages
- **Unread counts**: Redis counters per user per channel

---

### Question 3.3: Design Live Comments (YouTube Live / Twitch Chat)

| Category | Details |
|----------|---------|
| **Pattern** | Real-Time |
| **Sub-pattern** | High-Volume Live Updates |
| **Primary Challenge** | Massive concurrent writers (thousands of comments per second), fan-out to millions |
| **Read/Write Ratio** | Write-heavy with broadcast reads |
| **Consistency Needs** | Eventual (order approximate) |
| **Real-time** | Yes (sub-second) |
| **Key Technologies** | WebSockets, Kafka, Redis Streams |

**Key Decisions:**
- **Ingestion**: Kafka for buffering high-volume writes
- **Fan-out**: Redis Streams or Kafka per live event
- **Connection management**: WebSocket servers with topic subscriptions
- **Rate limiting**: Per-user rate limiting to prevent spam
- **Moderation**: Real-time filter service

---

### Question 3.4: Design Multiplayer Game Backend

| Category | Details |
|----------|---------|
| **Pattern** | Real-Time |
| **Sub-pattern** | Game State Synchronization |
| **Primary Challenge** | Ultra-low latency (50ms), state consistency, cheating prevention |
| **Read/Write Ratio** | Write-heavy (player actions) |
| **Consistency Needs** | Strong for game state |
| **Real-time** | Yes (UDP often) |
| **Key Technologies** | UDP, Game state servers, Deterministic lockstep, Authority server |

**Key Decisions:**
- **Protocol**: UDP for fast updates, TCP for reliable messages
- **State sync**: Server-authoritative model to prevent cheating
- **Prediction**: Client-side prediction with server reconciliation
- **Room management**: Dedicated game servers per match
- **Matchmaking**: Redis for player queues

---

# üü¢ PATTERN 4 ‚Äî Search Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Fast full-text search, relevance ranking, typo tolerance |
| **Typical Ratio** | Read-heavy (searches) |
| **Consistency** | Eventual (index freshness trade-offs) |
| **Key Components** | Inverted index, Search cluster, Indexers |
| **Sub-patterns** | Web Search, Product Search, Autocomplete |

---

### Question 4.1: Design Google Search

| Category | Details |
|----------|---------|
| **Pattern** | Search |
| **Sub-pattern** | Web-Scale Search |
| **Primary Challenge** | Crawling entire web, indexing petabytes, ranking, serving under 200ms |
| **Read/Write Ratio** | Read-heavy (searches) |
| **Consistency Needs** | Eventual (freshness vs quality trade-off) |
| **Real-time** | Yes (fast responses) |
| **Key Technologies** | Inverted index, PageRank, MapReduce, Distributed search |

**Key Components:**
1. **Crawler**: Downloads web pages, respects robots.txt
2. **Indexer**: Builds inverted index (word ‚Üí list of documents)
3. **Storage**: Bigtable for doc metadata, inverted index sharded
4. **Searcher**: Query parsing, ranking, snippet generation

**Key Decisions:**
- **Sharding**: Partition by document ID or term
- **Ranking**: TF-IDF, PageRank, machine learning
- **Freshness**: Incremental updates to index
- **Caching**: Query cache, result cache

---

### Question 4.2: Design YouTube Search

| Category | Details |
|----------|---------|
| **Pattern** | Search |
| **Sub-pattern** | Video Search |
| **Primary Challenge** | Searching video metadata and transcripts, ranking by views/engagement |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes |
| **Key Technologies** | Elasticsearch, Video metadata DB, Transcript index |

**Key Decisions:**
- **Index**: Elasticsearch for video titles, descriptions, tags
- **Transcripts**: Extract and index speech-to-text output
- **Ranking**: Weight by views, likes, recency
- **Filters**: Metadata filters (duration, upload date, quality)

---

### Question 4.3: Design Amazon Product Search

| Category | Details |
|----------|---------|
| **Pattern** | Search |
| **Sub-pattern** | E-commerce Search |
| **Primary Challenge** | Faceted search (filters by category, price, brand), inventory-aware results |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Strong for inventory (in-stock status) |
| **Real-time** | Yes |
| **Key Technologies** | Elasticsearch, Faceted indexing, Inventory cache |

**Key Decisions:**
- **Facets**: Store facet counts in index for fast filtering
- **Inventory**: Join with real-time inventory service
- **Ranking**: Relevance + conversion data + personalization
- **Synonyms**: Product synonym expansion

---

### Question 4.4: Design Search Autocomplete

| Category | Details |
|----------|---------|
| **Pattern** | Search |
| **Sub-pattern** | Typeahead Suggestions |
| **Primary Challenge** | Ultra-fast prefix matching, handling popularity, personalization |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes (sub-100ms) |
| **Key Technologies** | Trie (prefix tree), Frequency cache, Redis |

**Key Decisions:**
- **Data structure**: Trie (prefix tree) for fast prefix lookups
- **Ranking**: Weight suggestions by popularity (search frequency)
- **Personalization**: Boost based on user history
- **Freshness**: Update trie daily with recent search data
- **Caching**: Cache top suggestions for frequent prefixes

---

# üü¢ PATTERN 5 ‚Äî Rate Limiter / Throttling

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Enforcing API usage limits fairly and efficiently |
| **Typical Ratio** | Applied to all requests |
| **Consistency** | Strong (limits must be accurate) |
| **Key Components** | Redis, Token buckets, Distributed counters |
| **Sub-patterns** | Token Bucket, Leaky Bucket, Sliding Window |

---

### Question 5.1: Design API Rate Limiter

| Category | Details |
|----------|---------|
| **Pattern** | Rate Limiter |
| **Sub-pattern** | Distributed Rate Limiting |
| **Primary Challenge** | Accurate counting across multiple servers, low latency overhead |
| **Read/Write Ratio** | Every request is a write (to increment counter) |
| **Consistency Needs** | Strong (limits must be enforced) |
| **Real-time** | Yes (must not add significant latency) |
| **Key Technologies** | Redis, Lua scripts, Token bucket algorithm |

**Algorithms Comparison:**

| Algorithm | How It Works | Pros | Cons |
|-----------|--------------|------|------|
| **Token Bucket** | Tokens added at fixed rate, requests consume tokens | Simple, allows bursts | Memory per user |
| **Leaky Bucket** | Requests processed at fixed rate (queue) | Smooth output | No bursts |
| **Fixed Window** | Count requests in fixed time window | Simple | Edge case spikes |
| **Sliding Window** | Count in rolling time window | Accurate | More complex |
| **Sliding Window Log** | Track timestamps of all requests | Most accurate | Memory heavy |

**Key Decisions:**
- **Storage**: Redis with Lua for atomic operations
- **Rules**: Per-user, per-API, per-tier rate limits
- **Response**: Return headers with remaining quota
- **Distributed**: Consistent hashing to assign users to Redis nodes

---

### Question 5.2: Design API Gateway

| Category | Details |
|----------|---------|
| **Pattern** | Rate Limiter |
| **Sub-pattern** | Gateway with Multiple Features |
| **Primary Challenge** | Combining rate limiting, authentication, routing, logging |
| **Read/Write Ratio** | Every request touches gateway |
| **Consistency Needs** | Strong for auth |
| **Real-time** | Yes |
| **Key Technologies** | Reverse proxy (NGINX), Redis, JWT validation |

**Key Decisions:**
- **Layered approach**: Global rate limits + per-user limits
- **Authentication**: Validate JWT tokens, cache public keys
- **Routing**: Path-based routing to microservices
- **Logging**: Async logging to Kafka
- **Circuit breaking**: Detect failing downstream services

---

# üü¢ PATTERN 6 ‚Äî Logging / Metrics / Monitoring

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Ingesting high-volume time-series data, efficient querying |
| **Typical Ratio** | Write-heavy (constant data ingestion) |
| **Consistency** | Eventual (loss of some metrics acceptable) |
| **Key Components** | Time-series DB, Stream processor, Aggregator |
| **Sub-patterns** | Metrics, Logs, Tracing |

---

### Question 6.1: Design Metrics Monitoring System (Prometheus/Datadog)

| Category | Details |
|----------|---------|
| **Pattern** | Logging / Metrics |
| **Sub-pattern** | Time-Series Metrics |
| **Primary Challenge** | High-frequency data points, efficient storage, fast queries |
| **Read/Write Ratio** | Write-heavy (ingestion), read-spiky (dashboards) |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes (seconds delay) |
| **Key Technologies** | Time-series DB (InfluxDB/Prometheus), Downsampling, Aggregation |

**Key Decisions:**
- **Data model**: Metric name + labels/tags + timestamp + value
- **Ingestion**: HTTP endpoint, batch writes
- **Storage**: Time-series optimized (LSM tree, compression)
- **Downsampling**: Older data at lower resolution
- **Query**: PromQL-like language for aggregation
- **Alerting**: Rule evaluation engine

---

### Question 6.2: Design Log Aggregator (Splunk/ELK Stack)

| Category | Details |
|----------|---------|
| **Pattern** | Logging / Metrics |
| **Sub-pattern** | Log Ingestion and Search |
| **Primary Challenge** | High-volume log ingestion, full-text search, retention |
| **Read/Write Ratio** | Write-heavy (logs), read-spiky (troubleshooting) |
| **Consistency Needs** | Eventual |
| **Real-time** | Near-real-time (minutes) |
| **Key Technologies** | Log shippers (Fluentd), Kafka, Elasticsearch, Object storage |

**Architecture:**
```
Application ‚Üí Log Shipper ‚Üí Kafka ‚Üí Log Processor ‚Üí Elasticsearch
                                    ‚Üì
                              Object Storage (S3)
                                    ‚Üì
                              (Cold storage for older logs)
```

**Key Decisions:**
- **Shipping**: Lightweight agents on each server
- **Buffer**: Kafka for durability and backpressure
- **Indexing**: Elasticsearch for search
- **Retention**: Hot (ES), warm (ES with slower storage), cold (S3)
- **Search**: Kibana for visualization

---

### Question 6.3: Design Distributed Tracing (Jaeger/Zipkin)

| Category | Details |
|----------|---------|
| **Pattern** | Logging / Metrics |
| **Sub-pattern** | Request Tracing |
| **Primary Challenge** | Correlating spans across services, sampling strategies |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Near-real-time |
| **Key Technologies** | Span collectors, Trace storage, Sampling |

**Key Decisions:**
- **Trace context**: Propagate trace ID via headers
- **Sampling**: Head-based (sample at start) vs tail-based
- **Storage**: Cassandra for trace spans
- **UI**: Query by trace ID, service, duration

---

# üü¢ PATTERN 7 ‚Äî Distributed Caching Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Fast data access, consistency across nodes, high availability |
| **Typical Ratio** | Read-heavy |
| **Consistency** | Configurable (strong to eventual) |
| **Key Components** | Consistent hashing, Replication, Eviction |
| **Sub-patterns** | Key-Value Cache, CDN, Session Store |

---

### Question 7.1: Design Distributed Cache (Redis/Memcached)

| Category | Details |
|----------|---------|
| **Pattern** | Distributed Caching |
| **Sub-pattern** | In-Memory Key-Value Store |
| **Primary Challenge** | Data distribution, fault tolerance, eviction |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Configurable (eventual for scalability) |
| **Real-time** | Yes |
| **Key Technologies** | Consistent hashing, Replication, LRU/LFU |

**Key Decisions:**
- **Distribution**: Consistent hashing to minimize rehashing
- **Replication**: Master-slave for read scaling
- **Persistence**: Optional snapshot or AOF (Append-Only File)
- **Eviction**: LRU, LFU, TTL
- **Communication**: TCP with binary protocol

**Consistent Hashing:**
- Maps keys to nodes in a ring
- Adding/removing nodes only affects neighboring keys
- Virtual nodes for better distribution

---

### Question 7.2: Design CDN (Content Delivery Network)

| Category | Details |
|----------|---------|
| **Pattern** | Distributed Caching |
| **Sub-pattern** | Edge Caching |
| **Primary Challenge** | Geographic distribution, cache invalidation, origin offload |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes |
| **Key Technologies** | PoP (Points of Presence), Edge servers, Pull/Push strategies |

**Key Decisions:**
- **Pull CDN**: Edge fetches from origin on cache miss ‚Äî simpler
- **Push CDN**: Content pushed to edge proactively ‚Äî better control
- **Cache invalidation**: TTL, API-based purge
- **Routing**: GeoDNS to nearest PoP
- **SSL termination**: At edge for performance

---

### Question 7.3: Design Session Store

| Category | Details |
|----------|---------|
| **Pattern** | Distributed Caching |
| **Sub-pattern** | User Session Management |
| **Primary Challenge** | Session stickiness, expiration, security |
| **Read/Write Ratio** | Read and write on each request |
| **Consistency Needs** | Strong (user shouldn't be logged out unexpectedly) |
| **Real-time** | Yes |
| **Key Technologies** | Redis with TTL, JWT (alternative) |

**Key Decisions:**
- **Storage**: Redis with TTL for automatic expiration
- **Alternatives**: JWT (stateless but can't revoke)
- **Replication**: Redis Cluster for HA
- **Serialization**: JSON or protocol buffers

---

# üü¢ PATTERN 8 ‚Äî Recommendation Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Personalization, real-time updates, cold start problem |
| **Typical Ratio** | Read-heavy (recommendations) |
| **Consistency** | Eventual |
| **Key Components** | ML models, Feature store, Candidate generation |
| **Sub-patterns** | Collaborative Filtering, Content-Based, Hybrid |

---

### Question 8.1: Design Netflix Recommendation

| Category | Details |
|----------|---------|
| **Pattern** | Recommendation |
| **Sub-pattern** | Video Recommendations |
| **Primary Challenge** | Personalization at scale, new user cold start, diversity |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Near-real-time |
| **Key Technologies** | Matrix factorization, Spark, Cassandra, Feature store |

**Architecture:**
1. **Offline pipeline**: Train models daily (Spark, Hadoop)
2. **Nearline**: Real-time feature updates
3. **Online serving**: Pre-compute candidate sets
4. **Ranking**: ML model to rank candidates

**Key Decisions:**
- **Candidate generation**: Collaborative filtering, content-based, trending
- **Features**: Watch history, ratings, time of day, device
- **Cold start**: Popular content, demographic-based
- **Diversity**: Algorithm to avoid filter bubbles

---

### Question 8.2: Design YouTube Recommendation

| Category | Details |
|----------|---------|
| **Pattern** | Recommendation |
| **Sub-pattern** | Video Recommendations with Freshness |
| **Primary Challenge** | Balancing relevance with new content, watch time optimization |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes (new videos promoted) |
| **Key Technologies** | Two-stage model (candidate + ranking), TensorFlow, Bigtable |

**Key Decisions:**
- **Two-stage**: Generate hundreds of candidates ‚Üí rank with deep model
- **Objective**: Optimize for watch time, not clicks
- **Freshness**: Boost recent videos
- **Exploration**: Some randomization to discover new content

---

### Question 8.3: Design Amazon Product Recommendation

| Category | Details |
|----------|---------|
| **Pattern** | Recommendation |
| **Sub-pattern** | "Customers who bought this also bought" |
| **Primary Challenge** | Item-to-item collaborative filtering at scale |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Near-real-time (after purchase) |
| **Key Technologies** | Item co-occurrence matrix, DynamoDB, Pre-computation |

**Key Decisions:**
- **Algorithm**: Item-based collaborative filtering
- **Pre-computation**: Daily job to compute item similarities
- **Real-time**: Update with recent purchases
- **Personalization**: Also consider user history
- **Storage**: Fast lookup of similar items

---

# üü¢ PATTERN 9 ‚Äî Payment / Financial Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Transaction integrity, no double-spending, audit trail |
| **Typical Ratio** | Write-heavy (transactions) |
| **Consistency** | Strong (ACID required) |
| **Key Components** | SQL databases, Idempotency keys, Ledger |
| **Sub-patterns** | Payment Gateway, Wallet, UPI |

---

### Question 9.1: Design Payment Gateway (Stripe/PayPal)

| Category | Details |
|----------|---------|
| **Pattern** | Payment / Financial |
| **Sub-pattern** | Payment Processing |
| **Primary Challenge** | Idempotency, handling failures, PCI compliance |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Strong (money must be accurate) |
| **Real-time** | Yes |
| **Key Technologies** | SQL with transactions, Idempotency keys, Event sourcing |

**Key Decisions:**
- **Idempotency**: Client-provided idempotency key to prevent duplicates
- **State machine**: Payment states (created, processing, success, failed)
- **Double-entry ledger**: Every transaction has debit and credit entries
- **External calls**: Idempotent retries with backoff
- **Reconciliation**: Daily batch to match with banks

**Payment Flow:**
```
Client ‚Üí API Gateway ‚Üí Payment Service ‚Üí Ledger Service
                              ‚Üì
                        Fraud Detection
                              ‚Üì
                        Bank/Processor
                              ‚Üì
                        Webhook to Client
```

---

### Question 9.2: Design Wallet System

| Category | Details |
|----------|---------|
| **Pattern** | Payment / Financial |
| **Sub-pattern** | Digital Wallet |
| **Primary Challenge** | Balance consistency, concurrent transactions |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Strong |
| **Real-time** | Yes |
| **Key Technologies** | SQL with row-level locks, Optimistic concurrency |

**Key Decisions:**
- **Balance storage**: Row in SQL DB with version number
- **Concurrency**: Optimistic locking (compare-and-swap)
- **Transaction log**: Immutable log of all transactions
- **Reconciliation**: Periodic balance verification

---

### Question 9.3: Design UPI-like System (Unified Payments Interface)

| Category | Details |
|----------|---------|
| **Pattern** | Payment / Financial |
| **Sub-pattern** | Real-Time Payment |
| **Primary Challenge** | High throughput, low latency, interoperability between banks |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Strong (two-phase commit across banks) |
| **Real-time** | Yes (seconds) |
| **Key Technologies** | Distributed transactions, Message queues, Idempotency |

**Key Decisions:**
- **Two-phase commit**: Prepare ‚Üí Commit across banks
- **Fallback**: Compensation transactions on failure
- **Queue-based**: Async processing with idempotency
- **Settlement**: End-of-day net settlement between banks

---

# üü¢ PATTERN 10 ‚Äî Large-Scale Processing

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Processing massive datasets efficiently |
| **Typical Ratio** | Batch or streaming writes |
| **Consistency** | Varies by use case |
| **Key Components** | MapReduce, Stream processors, Data warehouses |
| **Sub-patterns** | Batch Processing, Stream Processing, ETL |

---

### Question 10.1: Design Analytics System (Google Analytics)

| Category | Details |
|----------|---------|
| **Pattern** | Large-Scale Processing |
| **Sub-pattern** | Web Analytics |
| **Primary Challenge** | Tracking billions of events, real-time dashboards, drill-down queries |
| **Read/Write Ratio** | Write-heavy (events) |
| **Consistency Needs** | Eventual |
| **Real-time** | Real-time dashboard + batch reports |
| **Key Technologies** | Kafka, Spark Streaming, Druid, HDFS |

**Architecture:**
```
Website/App ‚Üí Event Collector ‚Üí Kafka
                                 ‚Üì
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚Üì                              ‚Üì
           Stream Processor                Batch Processor
           (Real-time counts)               (Deep analytics)
                  ‚Üì                              ‚Üì
              Real-time DB                   Data Lake
              (Druid/Redis)                   (HDFS/S3)
                  ‚Üì                              ‚Üì
            Real-time Dashboard              Reporting DB
                                              (BigQuery)
```

**Key Decisions:**
- **Ingestion**: HTTP endpoint with compression
- **Buffer**: Kafka for durability
- **Real-time**: Stream processing for dashboard counts
- **Batch**: Hadoop/Spark for daily aggregations
- **Storage**: Columnar format (Parquet) for analytics

---

### Question 10.2: Design Clickstream Processing

| Category | Details |
|----------|---------|
| **Pattern** | Large-Scale Processing |
| **Sub-pattern** | User Behavior Pipeline |
| **Primary Challenge** | Sessionization, funnel analysis, user journey |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Near-real-time |
| **Key Technologies** | Kafka, Flink, Session windowing |

**Key Decisions:**
- **Session definition**: Timeout (30 min inactivity)
- **Processing**: Flink with session windows
- **Storage**: Clickhouse for fast queries
- **Output**: User behavior tables, funnels

---

### Question 10.3: Design Big Data Pipeline (ETL)

| Category | Details |
|----------|---------|
| **Pattern** | Large-Scale Processing |
| **Sub-pattern** | ETL Pipeline |
| **Primary Challenge** | Transforming raw data into structured formats, scheduling dependencies |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Strong (data quality) |
| **Real-time** | Batch (daily/hourly) |
| **Key Technologies** | Airflow, Spark, Data warehouse, Data lake |

**Key Decisions:**
- **Orchestration**: Airflow for DAG management
- **Processing**: Spark for transformations
- **Storage**: Data lake (raw) ‚Üí Data warehouse (structured)
- **Data quality**: Validation checks at each stage
- **Lineage**: Track data origins

---

### Question 10.4: Design Kafka-like System

| Category | Details |
|----------|---------|
| **Pattern** | Large-Scale Processing |
| **Sub-pattern** | Distributed Messaging System |
| **Primary Challenge** | Durable, ordered message storage, high throughput |
| **Read/Write Ratio** | Write and read at scale |
| **Consistency Needs** | Configurable (acks=all for strong) |
| **Real-time** | Yes |
| **Key Technologies** | Distributed commit log, Partitioning, Replication |

**Key Decisions:**
- **Storage**: Append-only log on disk
- **Partitioning**: Topic partitioned for parallelism
- **Replication**: Leader-follower per partition
- **Consumers**: Track offset per consumer group
- **Durability**: Sync to disk (configurable)

---

# üü¢ PATTERN 11 ‚Äî Geospatial / Location-Based Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Efficient spatial queries, real-time location updates |
| **Typical Ratio** | Write-heavy (location updates) |
| **Consistency** | Eventual (location can be slightly stale) |
| **Key Components** | Spatial indexing (Geohash, Quadtree), Real-time matching |
| **Sub-patterns** | Ride-sharing, Nearby search, Fleet tracking |

---

### Question 11.1: Design Uber / Lyft

| Category | Details |
|----------|---------|
| **Pattern** | Geospatial |
| **Sub-pattern** | Ride-Sharing |
| **Primary Challenge** | Matching riders with nearby drivers, real-time location updates, state management |
| **Read/Write Ratio** | Write-heavy (location pings), read-spiky (match requests) |
| **Consistency Needs** | Eventual for location, strong for trip state |
| **Real-time** | Yes |
| **Key Technologies** | Geohash, Redis, Kafka, State machines |

**Key Decisions:**
- **Location indexing**: Geohash or Quadtree to find nearby drivers
- **Matching**: Pub/sub to geohash cells
- **State management**: Trip states (requesting, matched, en-route, completed)
- **Surge pricing**: Real-time demand calculation
- **ETA calculation**: Routing engine with traffic data

**Geohash:**
- Encodes lat/long into string
- Longer prefix = higher precision
- Nearby points share prefix
- Query by prefix + neighbor cells

---

### Question 11.2: Design Google Maps / Nearby Places

| Category | Details |
|----------|---------|
| **Pattern** | Geospatial |
| **Sub-pattern** | Proximity Search |
| **Primary Challenge** | Finding places within radius, efficient spatial queries |
| **Read/Write Ratio** | Read-heavy |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes |
| **Key Technologies** | Geohash, Spatial databases (PostGIS), Quadtrees |

**Key Decisions:**
- **Indexing**: Geohash + inverted index for categories
- **Query**: Find geohash cells covering radius
- **Ranking**: Distance + relevance + popularity
- **Storage**: DynamoDB with geohash as partition key

---

### Question 11.3: Design Food Delivery App (DoorDash/Zomato)

| Category | Details |
|----------|---------|
| **Pattern** | Geospatial |
| **Sub-pattern** | Logistics + Discovery |
| **Primary Challenge** | Restaurant discovery, delivery tracking, assignment optimization |
| **Read/Write Ratio** | Read-heavy (browsing), write-spiky (orders) |
| **Consistency Needs** | Strong for orders, eventual for location |
| **Real-time** | Yes |
| **Key Technologies** | Geohash, Assignment service, Routing engine |

**Key Decisions:**
- **Discovery**: Geohash-based restaurant search
- **Assignment**: Nearest driver algorithm with capacity
- **Tracking**: Real-time location updates
- **Routing**: Optimized delivery routes

---

### Question 11.4: Design Nearby Friends Feature

| Category | Details |
|----------|---------|
| **Pattern** | Geospatial |
| **Sub-pattern** | Real-Time Location Sharing |
| **Primary Challenge** | Privacy, real-time updates, efficient proximity detection |
| **Read/Write Ratio** | Write-heavy (location pings) |
| **Consistency Needs** | Eventual |
| **Real-time** | Yes |
| **Key Technologies** | Geohash, WebSockets, Redis |

**Key Decisions:**
- **Privacy**: User controls who sees location
- **Frequency**: Adaptive update frequency based on movement
- **Detection**: Check if friends in same geohash cell
- **Storage**: Redis with TTL for current location

---

# üü¢ PATTERN 12 ‚Äî Collaborative Systems

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Concurrent editing, conflict resolution, real-time sync |
| **Typical Ratio** | Mixed reads and writes |
| **Consistency** | Strong (eventual with conflict resolution) |
| **Key Components** | CRDTs, Operational Transformation, WebSockets |
| **Sub-patterns** | Collaborative Editing, Whiteboard |

---

### Question 12.1: Design Google Docs

| Category | Details |
|----------|---------|
| **Pattern** | Collaborative Systems |
| **Sub-pattern** | Real-Time Collaborative Editor |
| **Primary Challenge** | Concurrent edits, conflict resolution, cursor presence |
| **Read/Write Ratio** | Write-heavy (keystrokes) |
| **Consistency Needs** | Strong convergence (all users see same document) |
| **Real-time** | Yes |
| **Key Technologies** | Operational Transformation (OT) or CRDTs, WebSockets, Version vectors |

**Key Decisions:**
- **Approach**: OT (Operational Transformation) or CRDT (Conflict-Free Replicated Data Type)
- **Operations**: Insert, delete, format
- **Transformation**: Transform concurrent ops to apply in any order
- **Presence**: Cursor positions broadcast via WebSockets
- **Storage**: Append-only op log + snapshot

**OT vs CRDT:**

| Approach | How It Works | Pros | Cons |
|----------|--------------|------|------|
| **OT** | Transform operations based on concurrent ops | Mature, used in Google Docs | Complex transformation logic |
| **CRDT** | Data structure that converges automatically | Mathematically sound | Higher overhead |

---

### Question 12.2: Design Figma / Collaborative Whiteboard

| Category | Details |
|----------|---------|
| **Pattern** | Collaborative Systems |
| **Sub-pattern** | Visual Collaboration |
| **Primary Challenge** | Vector graphics sync, low latency, performance |
| **Read/Write Ratio** | Write-heavy |
| **Consistency Needs** | Strong convergence |
| **Real-time** | Yes |
| **Key Technologies** | CRDTs, WebSockets, Canvas rendering |

**Key Decisions:**
- **Data model**: Scene graph of objects
- **Operations**: Move, resize, change properties
- **Rendering**: Client-side canvas rendering
- **Optimization**: Throttle updates, batch operations

---

# üü¢ PATTERN 13 ‚Äî Infrastructure / System Utilities

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Building foundational systems that other services depend on |
| **Typical Ratio** | Varies |
| **Consistency** | Often strong |
| **Key Components** | Distributed coordination, Leader election |
| **Sub-patterns** | Job Scheduler, Distributed Lock, ID Generator |

---

### Question 13.1: Design Distributed Job Scheduler

| Category | Details |
|----------|---------|
| **Pattern** | Infrastructure |
| **Sub-pattern** | Cron at Scale |
| **Primary Challenge** | Exactly-once execution, fault tolerance, time accuracy |
| **Read/Write Ratio** | Write on schedule, read on execution |
| **Consistency Needs** | Strong (job must run exactly once) |
| **Real-time** | Scheduled |
| **Key Technologies** | Leader election (ZooKeeper), Distributed locks, Quartz |

**Key Decisions:**
- **Leader election**: One scheduler active, others standby
- **Job storage**: SQL with row locks
- **Missed jobs**: Catch-up logic
- **Distributed execution**: Workers pick jobs from queue
- **Idempotency**: Jobs designed to be idempotent

---

### Question 13.2: Design Distributed Locking Service

| Category | Details |
|----------|---------|
| **Pattern** | Infrastructure |
| **Sub-pattern** | Distributed Coordination |
| **Primary Challenge** | Deadlock prevention, fault tolerance |
| **Read/Write Ratio** | Write on lock/unlock |
| **Consistency Needs** | Strong |
| **Real-time** | Yes |
| **Key Technologies** | Redis Redlock, ZooKeeper, Chubby |

**Key Decisions:**
- **Algorithm**: Redis Redlock or ZooKeeper sequential znodes
- **Lease**: Locks with TTL to handle holder failure
- **Fencing tokens**: Prevent stale lock holders
- **Reentrancy**: Same client can lock multiple times

---

### Question 13.3: Design Unique ID Generator (Snowflake)

| Category | Details |
|----------|---------|
| **Pattern** | Infrastructure |
| **Sub-pattern** | Distributed ID Generation |
| **Primary Challenge** | Unique, time-sortable, high throughput |
| **Read/Write Ratio** | Write (generate ID per request) |
| **Consistency Needs** | Strong (no duplicates) |
| **Real-time** | Yes |
| **Key Technologies** | Snowflake algorithm, ZooKeeper for worker IDs |

**Snowflake ID Structure (64-bit):**
```
| Timestamp (41 bits) | Worker ID (10 bits) | Sequence (12 bits) |
```

**Key Decisions:**
- **Timestamp**: Milliseconds since custom epoch
- **Worker ID**: Unique per generator (via ZooKeeper)
- **Sequence**: Increments within same millisecond
- **Synchronization**: NTP for time sync

---

# üü¢ PATTERN 14 ‚Äî Web Crawler / Search Indexing

### Pattern Overview

| Attribute | Description |
|-----------|-------------|
| **Core Challenge** | Polite crawling, deduplication, scale |
| **Typical Ratio** | Write-heavy (download pages) |
| **Consistency** | Eventual |
| **Key Components** | URL frontier, Deduplication, Rate limiting |
| **Sub-patterns** | Web Crawler, Focused Crawler |

---

### Question 14.1: Design Web Crawler

| Category | Details |
|----------|---------|
| **Pattern** | Web Crawler |
| **Sub-pattern** | Distributed Crawling |
| **Primary Challenge** | Politeness (don't hammer sites), deduplication, freshness |
| **Read/Write Ratio** | Write-heavy (downloading) |
| **Consistency Needs** | Eventual |
| **Real-time** | No (batch) |
| **Key Technologies** | URL frontier (priority queue), Bloom filters, Robots.txt parser |

**Architecture:**
```
Seed URLs ‚Üí URL Frontier (Priority Queue)
                ‚Üì
         Fetch Service
                ‚Üì
        Parse & Extract
                ‚Üì
         Content Store
                ‚Üì
       URL Deduplication (Bloom Filter)
                ‚Üì
         New URLs to Frontier
```

**Key Decisions:**
- **Politeness**: Delay between requests to same domain
- **Deduplication**: Bloom filter for URL seen test
- **Prioritization**: Crawl important pages first (PageRank)
- **Freshness**: Re-crawl based on change frequency
- **Robots.txt**: Respect crawling rules

---

# üìÖ PHASE 4 ‚Äî 8-Week Execution Plan

## Week-by-Week Breakdown

| Week | Focus | Key Activities |
|------|-------|----------------|
| **Week 1-2** | Core Fundamentals | Study networking, databases, CAP theorem, caching, messaging |
| **Week 3** | CRUD + Storage Systems | Practice URL Shortener, Pastebin, Dropbox, Ticket Booking |
| **Week 4** | Feed Systems | Practice Twitter, Instagram, Facebook Feed, LinkedIn |
| **Week 5** | Real-Time + Search | Practice WhatsApp, Discord, Google Search, Autocomplete |
| **Week 6** | Rate Limiter + Caching + Logging | Practice Rate Limiter, Distributed Cache, Metrics System |
| **Week 7** | Payments + Recommendation | Practice Payment Gateway, Netflix Recs, Uber |
| **Week 8** | Mock Interviews | Full mock interviews with feedback |

## Daily Practice Routine

| Time | Activity |
|------|----------|
| **30 min** | Review one concept (caching strategies, consistency models) |
| **1 hour** | Solve one design problem using the 6-step framework |
| **15 min** | Document in tracker (what went well, what to improve) |
| **Weekly** | One full mock interview with peer |

---

# üéØ How to Practice (Like Your DP Excel Tracker)

Since you love structured tracking, create this Excel/Notion tracker:

## HLD Practice Tracker Template

| Question | Pattern | Sub-pattern | Primary Challenge | Read/Write | Consistency | Attempt 1 Date | Bottlenecks Missed | Key Improvements | Status |
|----------|---------|-------------|-------------------|------------|-------------|----------------|-------------------|-------------------|--------|
| Design Twitter | Feed | Social Media | Celebrity fan-out | Heavy Read | Eventual | 2024-01-15 | Hotkey partitioning | Add hybrid fan-out | üü° |
| Design WhatsApp | Real-Time | Messaging | Message ordering | Write-heavy | Strong ordering | 2024-01-16 | Last seen scaling | Redis with TTL | üü¢ |

**Color Coding:**
- üü¢ **Green** = Confident (can explain trade-offs, handled follow-ups well)
- üü° **Yellow** = Needs improvement (missed key bottlenecks)
- üî¥ **Red** = Weak (struggled with fundamentals)

---

# üß† How to Think Like Top Engineers

## The 5 Critical Questions to Always Ask

1. **"Where does this break?"** ‚Äî Identify single points of failure
2. **"What if traffic becomes 10x?"** ‚Äî Think about scaling bottlenecks
3. **"What if one region fails?"** ‚Äî Consider disaster recovery
4. **"What if DB crashes?"** ‚Äî Plan for data layer failures
5. **"What is the biggest bottleneck?"** ‚Äî Prioritize improvements

## Trade-off Mindset

At every decision point, ask:
- "Why X over Y?"
- "What do we gain? What do we lose?"
- "Under what conditions would the other choice be better?"

---

# ü•á Top 20 Must-Do Questions (Most Frequently Asked)

| Rank | Question | Pattern | Why It's Asked |
|------|----------|---------|----------------|
| 1 | Design Twitter/X | Feed | Tests fan-out, caching, timeline generation |
| 2 | Design WhatsApp | Real-Time | Tests WebSockets, message ordering, presence |
| 3 | Design URL Shortener | CRUD | Tests hashing, key generation, redirection |
| 4 | Design YouTube | Streaming | Tests CDN, transcoding, large file storage |
| 5 | Design Uber | Geospatial | Tests location indexing, real-time matching |
| 6 | Design Instagram | Feed + Media | Tests image storage, feed ranking |
| 7 | Design Dropbox | Storage | Tests file sync, deduplication, delta sync |
| 8 | Design Google Drive | Storage | Similar to Dropbox with collaboration |
| 9 | Design API Rate Limiter | Rate Limiter | Tests algorithms, distributed counting |
| 10 | Design Distributed Cache | Caching | Tests consistent hashing, eviction |
| 11 | Design Payment System | Financial | Tests idempotency, ACID, double-entry |
| 12 | Design Ticket Booking | CRUD + Concurrency | Tests locking, concurrency control |
| 13 | Design Netflix | Streaming + Recs | Tests CDN, recommendation systems |
| 14 | Design Google Maps | Geospatial | Tests spatial indexing, routing |
| 15 | Design Facebook News Feed | Feed | Tests algorithmic ranking, diverse content |
| 16 | Design Chat System | Real-Time | Tests WebSockets, message persistence |
| 17 | Design Search Autocomplete | Search | Tests trie, prefix matching, popularity |
| 18 | Design Distributed Logging | Logging | Tests high-volume ingestion, search |
| 19 | Design Metrics Monitoring | Logging | Tests time-series DB, aggregation |
| 20 | Design Kafka-like System | Large-Scale | Tests distributed log, partitioning |

---

# üî• Advanced (Staff-Level) Topics

Once you're comfortable with standard designs, dive into these advanced topics:

## Multi-Region Architecture

| Pattern | Description | Trade-offs |
|---------|-------------|------------|
| **Active-Passive** | One region active, other on standby | Simple, but failover takes time |
| **Active-Active** | All regions serving traffic | Complex, needs global consensus |
| **Geo-sharding** | Data partitioned by region | Reduced latency, compliance |

## Data Migration Strategies

| Strategy | When to Use |
|----------|-------------|
| **Blue-Green Deployment** | Zero-downtime migrations |
| **Parallel Runs** | Validate new system before cutover |
| **Gradual Rollout** | Migrate users in batches |
| **Backfill** | Migrate historical data after cutover |

## Schema Evolution

| Technique | Description |
|-----------|-------------|
| **Backward Compatible** | New code can read old data |
| **Forward Compatible** | Old code can read new data |
| **Versioning** | Store schema version with data |
| **Dual Writes** | Write to both old and new schemas during migration |

## Consistent Hashing Internals

- Virtual nodes for better distribution
- Replication factor for fault tolerance
- Read repair for consistency

## Leader Election Algorithms

| Algorithm | How It Works |
|-----------|--------------|
| **Bully Algorithm** | Highest ID node becomes leader |
| **Raft** | Election terms, majority vote |
| **Paxos** | Complex, used in Chubby/ZooKeeper |
| **ZooKeeper** | Sequential znodes for leader election |

## Consensus Protocols

- **Paxos**: Classic but complex
- **Raft**: Understandable, used in etcd
- **Zab**: ZooKeeper's protocol

## Advanced CAP Scenarios

- **Network partition handling** ‚Äî What does your system do?
- **Tunable consistency** ‚Äî Cassandra's consistency levels
- **Quorum-based systems** ‚Äî Read/write quorums

---

# üìö Complete Resource Guide

## Books (Must Read)

| Book | Why |
|------|-----|
| **Designing Data-Intensive Applications** | The bible ‚Äî read chapters on replication, partitioning, consistency |
| **System Design Interview ‚Äî An Insider's Guide (Vol 1)** | Best practical introduction |
| **System Design Interview ‚Äî An Insider's Guide (Vol 2)** | More advanced problems |
| **Building Microservices** | Microservices architecture patterns |
| **Database Internals** | Deep dive into storage engines |

## Online Courses

| Course | Platform |
|--------|----------|
| **Grokking the System Design Interview** | DesignGurus |
| **System Design Interview** | Exponent |
| **Hello Interview** | HelloInterview (with AI mock interviews) |

## YouTube Channels

| Channel | Best For |
|---------|----------|
| **Gaurav Sen** | System design concepts |
| **CodeKarle** | Detailed walkthroughs |
| **System Design Interview** | Mock interviews |
| **Hussein Nasser** | Networking and databases |

## Blogs and Websites

| Site | Content |
|------|---------|
| **High Scalability** | Real-world architecture case studies |
| **The Architect Elevator** | Architecture patterns |
| **Martin Fowler's Blog** | Microservices, architecture |
| **AWS Architecture Blog** | Cloud-native patterns |

## Practice Platforms

| Platform | Features |
|----------|----------|
| **Pramp** | Free peer mock interviews |
| **Interviewing.io** | Anonymous mock interviews with engineers |
| **HelloInterview** | AI-powered mock interviews |

---

# üéØ Final Advice for You Specifically

Since you're:

- ‚úÖ Strong in pattern-based learning (DSA + DP)
- ‚úÖ Structured in tracking progress
- ‚úÖ Preparing seriously for interviews

## Your Success Formula

1. **Treat HLD exactly like DP** ‚Äî Identify patterns, solve 3-5 per pattern, review trade-offs, repeat until fluent

2. **Use your tracker religiously** ‚Äî Document every attempt, what you missed, and improvements

3. **Focus on trade-offs** ‚Äî The interviewer cares more about *why* you chose something than *what* you chose

4. **Practice vocalizing** ‚Äî Design is collaborative; practice explaining your thinking out loud

5. **Mock interviews are non-negotiable** ‚Äî At least 5 full mocks before the real interview

## Common Pitfalls to Avoid

| Pitfall | How to Avoid |
|---------|--------------|
| Jumping straight to design | Always clarify requirements first |
| Ignoring non-functional requirements | Ask about scale, latency, consistency |
| Memorizing solutions | Learn patterns, not specific designs |
| Not identifying bottlenecks | Always ask "what breaks?" |
| One-size-fits-all database | Justify SQL vs NoSQL choice |

---

# ‚úÖ Quick Reference: HLD Interview Checklist

## Pre-Interview Checklist
- [ ] Review core concepts (CAP, caching, databases)
- [ ] Refresh 5 most common patterns
- [ ] Practice 1 design problem (timed)
- [ ] Prepare questions for interviewer

## During Interview Checklist
- [ ] Step 1: Clarify requirements (5 min)
- [ ] Step 2: Estimate scale (5 min)
- [ ] Step 3: Design APIs (5 min)
- [ ] Step 4: High-level diagram (10 min)
- [ ] Step 5: Deep dive (15 min)
- [ ] Step 6: Trade-offs and bottlenecks (5 min)

## Post-Interview Reflection
- [ ] What requirements did I miss?
- [ ] What bottlenecks did I overlook?
- [ ] What trade-offs could I have discussed better?
- [ ] Update tracker with learnings

---

# üöÄ Your Next Steps

1. **Start with Phase 1** ‚Äî Build foundations (2 weeks)
2. **Create your tracker** ‚Äî Excel/Notion with all questions
3. **Pick first pattern (CRUD)** ‚Äî Solve 3 questions this week
4. **Find a mock partner** ‚Äî Pramp or study group
5. **Schedule weekly reviews** ‚Äî Track progress

---

**Remember:** The goal is not to memorize architectures but to develop a **trade-off mindset**. Every design decision is a balance between competing concerns. Show the interviewer you understand these trade-offs and can make reasonable choices under ambiguity.

You've got this, Shubham! Your pattern-based approach from DSA will serve you well here. Just apply the same discipline, and you'll master HLD.

---

**Want to proceed? Tell me:**
1. Do you want the Excel/Notion tracker template?
2. Shall we do a mock deep-dive on one pattern (like Feed systems)?
3. Or start with the first pattern (CRUD) with a guided solution?
